{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "### Idea\n",
    "Because training base models is hard, the possible optimal solution may be to take a lot of classical models\n",
    "for portfolio creation and train a 2nd-layer model on their outputs, to select an optimal output.\n",
    "\n",
    "The challenges we need to solve in this notebook are the following:\n",
    "- selecting a target variable and metric\n",
    "- generating as many simple models as possible\n",
    "- adding features related to the long-term trends for 2nd layer model for it to perform a \"more informed\" choice\n",
    "- choosing whether base models should be allowed to see entire past that we have or just the most recent fraction of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.data_loader import load_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AP</th>\n",
       "      <th>ARR</th>\n",
       "      <th>ARW</th>\n",
       "      <th>G</th>\n",
       "      <th>OP</th>\n",
       "      <th>ORR</th>\n",
       "      <th>ORW</th>\n",
       "      <th>a5.c</th>\n",
       "      <th>wig2</th>\n",
       "      <th>^aex</th>\n",
       "      <th>...</th>\n",
       "      <th>SEK</th>\n",
       "      <th>CHF</th>\n",
       "      <th>THB</th>\n",
       "      <th>TTD</th>\n",
       "      <th>TND</th>\n",
       "      <th>AED</th>\n",
       "      <th>GBP</th>\n",
       "      <th>USD</th>\n",
       "      <th>UYU</th>\n",
       "      <th>VEB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>415.90</td>\n",
       "      <td>549.11</td>\n",
       "      <td>354.45</td>\n",
       "      <td>401.26</td>\n",
       "      <td>275.08</td>\n",
       "      <td>520.13</td>\n",
       "      <td>230.72</td>\n",
       "      <td>1204.88</td>\n",
       "      <td>1852.9</td>\n",
       "      <td>675.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085771</td>\n",
       "      <td>0.456726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.115867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.726696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>404.41</td>\n",
       "      <td>533.89</td>\n",
       "      <td>357.14</td>\n",
       "      <td>401.42</td>\n",
       "      <td>275.08</td>\n",
       "      <td>520.02</td>\n",
       "      <td>229.63</td>\n",
       "      <td>1194.41</td>\n",
       "      <td>1796.6</td>\n",
       "      <td>642.25</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.465253</td>\n",
       "      <td>0.019568</td>\n",
       "      <td>0.115445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197034</td>\n",
       "      <td>1.18701</td>\n",
       "      <td>0.723608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>400.04</td>\n",
       "      <td>527.38</td>\n",
       "      <td>351.19</td>\n",
       "      <td>401.59</td>\n",
       "      <td>275.08</td>\n",
       "      <td>519.22</td>\n",
       "      <td>229.22</td>\n",
       "      <td>1192.89</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>632.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086740</td>\n",
       "      <td>0.466615</td>\n",
       "      <td>0.019422</td>\n",
       "      <td>0.115510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197039</td>\n",
       "      <td>1.18624</td>\n",
       "      <td>0.723627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>410.15</td>\n",
       "      <td>522.02</td>\n",
       "      <td>347.96</td>\n",
       "      <td>401.75</td>\n",
       "      <td>275.07</td>\n",
       "      <td>519.62</td>\n",
       "      <td>228.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1832.1</td>\n",
       "      <td>624.21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.468650</td>\n",
       "      <td>0.019427</td>\n",
       "      <td>0.115662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197260</td>\n",
       "      <td>1.19474</td>\n",
       "      <td>0.724439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>429.16</td>\n",
       "      <td>533.16</td>\n",
       "      <td>351.87</td>\n",
       "      <td>401.93</td>\n",
       "      <td>275.07</td>\n",
       "      <td>520.80</td>\n",
       "      <td>230.09</td>\n",
       "      <td>1223.61</td>\n",
       "      <td>1933.2</td>\n",
       "      <td>644.86</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.465233</td>\n",
       "      <td>0.019410</td>\n",
       "      <td>0.115876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197989</td>\n",
       "      <td>1.19596</td>\n",
       "      <td>0.727113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AP     ARR     ARW       G      OP     ORR     ORW     a5.c  \\\n",
       "2000-01-03  415.90  549.11  354.45  401.26  275.08  520.13  230.72  1204.88   \n",
       "2000-01-04  404.41  533.89  357.14  401.42  275.08  520.02  229.63  1194.41   \n",
       "2000-01-05  400.04  527.38  351.19  401.59  275.08  519.22  229.22  1192.89   \n",
       "2000-01-06  410.15  522.02  347.96  401.75  275.07  519.62  228.82      NaN   \n",
       "2000-01-07  429.16  533.16  351.87  401.93  275.07  520.80  230.09  1223.61   \n",
       "\n",
       "              wig2    ^aex  ...       SEK       CHF       THB       TTD  TND  \\\n",
       "2000-01-03  1852.9  675.44  ...  0.085771  0.456726       NaN  0.115867  NaN   \n",
       "2000-01-04  1796.6  642.25  ...       NaN  0.465253  0.019568  0.115445  NaN   \n",
       "2000-01-05  1777.0  632.31  ...  0.086740  0.466615  0.019422  0.115510  NaN   \n",
       "2000-01-06  1832.1  624.21  ...       NaN  0.468650  0.019427  0.115662  NaN   \n",
       "2000-01-07  1933.2  644.86  ...       NaN  0.465233  0.019410  0.115876  NaN   \n",
       "\n",
       "                 AED      GBP       USD  UYU       VEB  \n",
       "2000-01-03  0.197875      NaN  0.726696  NaN       NaN  \n",
       "2000-01-04  0.197034  1.18701  0.723608  NaN  0.001114  \n",
       "2000-01-05  0.197039  1.18624  0.723627  NaN  0.001114  \n",
       "2000-01-06  0.197260  1.19474  0.724439  NaN  0.001115  \n",
       "2000-01-07  0.197989  1.19596  0.727113  NaN  0.001118  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_all()\n",
    "df = df.loc[~df['AP'].isna()]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4801, 200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_colnames = ['AP', 'ARR', 'ARW', 'G', 'OP', 'ORR', 'ORW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "funds_df = df[fund_colnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4801, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funds_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and target selection\n",
    "\n",
    "Firstly, we will try to train a classifier that selects the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "from pypfopt.expected_returns import mean_historical_return\n",
    "from pypfopt.risk_models import CovarianceShrinkage\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default number of trading days in a year=252, month=21, week=5\n",
      "Using volatility threshold=0.1\n"
     ]
    }
   ],
   "source": [
    "year_days = int(funds_df.groupby(funds_df.index.year).count().mean().mean())\n",
    "month_days = int(year_days/12)\n",
    "week_days = int(month_days/4)\n",
    "print(f\"Using default number of trading days in a year={year_days}, month={month_days}, week={week_days}\")\n",
    "\n",
    "max_volatility = 0.1\n",
    "print(f\"Using volatility threshold={max_volatility}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Portfolio(object):\n",
    "    AP: float\n",
    "    ARR: float\n",
    "    ARW: float\n",
    "    G: float\n",
    "    OP: float\n",
    "    ORR: float\n",
    "    ORW: float\n",
    "\n",
    "    def as_weights(self):\n",
    "        return np.array(list(asdict(self).values()))\n",
    "\n",
    "\n",
    "def risk_free_rate(table, interval_len_in_days: int=1) -> int:\n",
    "    \"\"\" For the purpose of this portfolio selection, we use G (cash fund returns) as a risk-free rate. \"\"\"\n",
    "    intervals_in_a_year = year_days / interval_len_in_days\n",
    "    return np.mean((df['G'].shift(1) - df['G'].iloc[:-1]) / df['G'].iloc[:-1]) * intervals_in_a_year\n",
    "\n",
    "\n",
    "def _ef_builder(table):\n",
    "    \"\"\" Builder for all kinds of efficient frontier models. \"\"\"\n",
    "    table = table.copy().groupby(by=[table.index.year, table.index.month]).tail(n=1)\n",
    "    mu = mean_historical_return(table)\n",
    "    S = CovarianceShrinkage(table).ledoit_wolf()\n",
    "    return EfficientFrontier(mu, S)\n",
    "\n",
    "def _ef_meta_builder(ef):\n",
    "    \"\"\" \n",
    "    Common metadata for all kinds of efficient frontier models:\n",
    "    expected annualized returns, expected volatility, sharpe ratio\n",
    "    \"\"\"\n",
    "    return np.array(ef.portfolio_performance())\n",
    "\n",
    "def ef_max_sharpe(table, interval: int=1):\n",
    "    ef = _ef_builder(table)\n",
    "    weights = ef.max_sharpe(risk_free_rate(interval))\n",
    "    return Portfolio(**weights), _ef_meta_builder(ef)\n",
    "\n",
    "def ef_min_volatility(table, interval: int=1):\n",
    "    ef = _ef_builder(table)\n",
    "    weights = ef.min_volatility()\n",
    "    return Portfolio(**weights), _ef_meta_builder(ef)\n",
    "\n",
    "def ef_efficient_risk(table, risk_target: float, interval: int=1):\n",
    "    ef = _ef_builder(table)\n",
    "    weights = ef.efficient_risk(risk_target, risk_free_rate=risk_free_rate(interval))\n",
    "    return Portfolio(**weights), _ef_meta_builder(ef)\n",
    "\n",
    "def ef_efficient_return(table, target_return, interval: int=1):\n",
    "    ef = _ef_builder(table)\n",
    "    weights = ef.efficient_return(target_return)\n",
    "    return Portfolio(**weights), _ef_meta_builder(ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(funds_df: pd.DataFrame) -> Tuple[np.array, List[Portfolio]]:\n",
    "    \"\"\"\n",
    "    Returns features for the model along with the portfolios that were used for calculating these features.\n",
    "    \"\"\"\n",
    "    monthly_table = funds_df.groupby([funds_df.index.year, funds_df.index.month]).tail(1)\n",
    "    weekly_table = funds_df.groupby([funds_df.index.year, funds_df.index.week]).tail(1)\n",
    "    portfolio_data = [\n",
    "        ef_max_sharpe(funds_df),\n",
    "        ef_max_sharpe(monthly_table, month_days),\n",
    "        ef_max_sharpe(weekly_table, week_days),\n",
    "        ef_min_volatility(funds_df),\n",
    "        ef_min_volatility(monthly_table, month_days),\n",
    "        ef_min_volatility(weekly_table, week_days),\n",
    "        ef_efficient_risk(funds_df, 0.10),\n",
    "        ef_efficient_risk(funds_df, 0.05),\n",
    "        ef_efficient_risk(monthly_table, 0.10, month_days),\n",
    "        ef_efficient_risk(monthly_table, 0.05, month_days),\n",
    "        ef_efficient_risk(weekly_table, 0.10, week_days),\n",
    "        ef_efficient_risk(weekly_table, 0.05, week_days),\n",
    "        ef_efficient_return(funds_df, 0.04),\n",
    "        ef_efficient_return(funds_df, 0.08),\n",
    "        ef_efficient_return(monthly_table, 0.04, month_days),\n",
    "        ef_efficient_return(monthly_table, 0.08, month_days),\n",
    "        ef_efficient_return(weekly_table, 0.04, week_days),\n",
    "        ef_efficient_return(weekly_table, 0.08, week_days),\n",
    "    ]\n",
    "    portfolios = [pd[0] for pd in portfolio_data]\n",
    "    portfolio_weights = np.hstack([p.as_weights() for p in portfolios])\n",
    "    portfolio_features = np.hstack(pd[1] for pd in portfolio_data)\n",
    "    return np.hstack([portfolio_weights, portfolio_features]), portfolios\n",
    "    \n",
    "\n",
    "@jit(nopython=True)\n",
    "def portfolio_performance(portfolio_allocation: np.array, fund_df_values: np.array) -> Tuple[float, float]:\n",
    "    fund_returns = (fund_df_values[-1] - fund_df_values[0]) / fund_df_values[0]\n",
    "    portfolio_returns = np.sum(portfolio_allocation*fund_returns) / np.sum(portfolio_allocation)\n",
    "    # eliminating portfolios based on too high volatility\n",
    "    portfolio_period_values = np.sum(portfolio_allocation*fund_df_values[:-1], axis=1) / np.sum(portfolio_allocation)\n",
    "    portfolio_period_shifted = np.sum(portfolio_allocation*fund_df_values[1:], axis=1) / np.sum(portfolio_allocation)\n",
    "    portfolio_daily_returns = (portfolio_period_shifted - portfolio_period_values) / portfolio_period_values\n",
    "    portfolio_volatility = np.var(portfolio_daily_returns)\n",
    "    if portfolio_volatility > max_volatility:\n",
    "        print(\"Exceeded max volatility:\", portfolio_volatility)\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return portfolio_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180,) 18 Portfolio(AP=2.8675560682935483e-16, ARR=0.0241814682778109, ARW=0.0, G=0.6310321596791412, OP=0.18473105611147625, ORR=0.13726016068669422, ORW=0.0227951552448774)\n",
      "CPU times: user 1.43 s, sys: 0 ns, total: 1.43 s\n",
      "Wall time: 1.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features, pfs = calculate_features(funds_df)\n",
    "print(features.shape, len(pfs), pfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.334257975034674"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_performance(np.array([0,0,0,0,0,0,1]), funds_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_X_y(\n",
    "        funds_df: pd.DataFrame,\n",
    "        n_features: int,\n",
    "        min_test_idx: int=5*year_days, \n",
    "        test_len: int=year_days\n",
    "    ):\n",
    "    max_test_idx = len(funds_df)\n",
    "    n_samples = max_test_idx - min_test_idx - test_len\n",
    "    X = np.zeros((n_samples, n_features))\n",
    "    y = np.zeros(n_samples)\n",
    "    sample_weights = np.zeros(n_samples)\n",
    "    for idx in tqdm(range(n_samples)):\n",
    "        test_starting_idx = min_test_idx + idx\n",
    "        present_data = funds_df.iloc[:test_starting_idx]\n",
    "        test_data = funds_df.iloc[test_starting_idx:test_starting_idx+test_len]\n",
    "        X[idx], portfolios = calculate_features(present_data)\n",
    "        portfolio_performances = np.array([portfolio_performance(p.as_weights(), test_data.values) for p in portfolios])\n",
    "        y[idx] = np.argmax(portfolio_performances)\n",
    "        sample_weights[idx] = 1 + np.max(portfolio_performances) - np.mean(portfolio_performances[portfolio_performances > -np.inf])\n",
    "    return X, y, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3289/3289 [1:31:19<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "X, y, sample_weights = compute_X_y(funds_df, n_features=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3289, 180), (3289,), dtype('float64'), dtype('int64'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, X.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18-funds-data-2019-03-09\n",
      "CPU times: user 0 ns, sys: 12 ms, total: 12 ms\n",
      "Wall time: 8.64 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filename = \"18-funds-data-\" + str(pd.datetime.now().date())\n",
    "print(filename)\n",
    "np.save(f'data/{filename}-X.npy', X)\n",
    "np.save(f'data/{filename}-y.npy', y)\n",
    "np.save(f'data/{filename}-weights.npy', sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16    744\n",
       "13    553\n",
       "12    550\n",
       "17    538\n",
       "0     433\n",
       "2     242\n",
       "6     118\n",
       "10     76\n",
       "5      14\n",
       "7      10\n",
       "3      10\n",
       "11      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not shuffling during the test set separation in order not to let the model know about macro trends that might appear in the future and are not captured within our set of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2631, 180), (2631,), (658, 180), (658,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2104, 180), (2104,), (527, 180), (527,), (658, 180), (658,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_train = sample_weights[:y_train.shape[0]]\n",
    "weights_val = sample_weights[y_train.shape[0]:y_train.shape[0]+y_val.shape[0]]\n",
    "weights_test = sample_weights[y_train.shape[0]+y_val.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the classes will never be predicted because they are not present in the training set.\n",
    "\n",
    "For now, we will just remove the rarely occuring values from the entire dataset and work on the ones that can be split efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  2,  3,  5,  6, 10, 11, 12, 13, 16, 17]),\n",
       " array([ 0,  3,  7, 12, 13, 16, 17]),\n",
       " array([ 0,  2,  6,  7, 12, 13, 16, 17]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train), np.unique(y_val), np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524, 180) (524,) (524,)\n"
     ]
    }
   ],
   "source": [
    "keep_indices = np.isin(y_val, np.unique(y_train))\n",
    "X_val, y_val, weights_val = X_val[keep_indices], y_val[keep_indices], weights_val[keep_indices]\n",
    "print(X_val.shape, y_val.shape, weights_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "#     Learning params:\n",
    "    'num_leaves': 32,\n",
    "    'learning_rate': 0.0012137,\n",
    "    'n_estimators': 4096,\n",
    "#     Regularization:\n",
    "    'lambda_l1': 0.25,\n",
    "#     Technical:\n",
    "    'silent': True,\n",
    "    'n_jobs': 4,\n",
    "    'num_class': 18\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 128 rounds.\n",
      "[256]\tvalid_0's multi_logloss: 1.67704\tvalid_0's multi_logloss: 1.67704\n",
      "[512]\tvalid_0's multi_logloss: 1.63565\tvalid_0's multi_logloss: 1.63565\n",
      "[768]\tvalid_0's multi_logloss: 1.60535\tvalid_0's multi_logloss: 1.60535\n",
      "[1024]\tvalid_0's multi_logloss: 1.58497\tvalid_0's multi_logloss: 1.58497\n",
      "[1280]\tvalid_0's multi_logloss: 1.5629\tvalid_0's multi_logloss: 1.5629\n",
      "[1536]\tvalid_0's multi_logloss: 1.54343\tvalid_0's multi_logloss: 1.54343\n",
      "[1792]\tvalid_0's multi_logloss: 1.53339\tvalid_0's multi_logloss: 1.53339\n",
      "Early stopping, best iteration is:\n",
      "[1772]\tvalid_0's multi_logloss: 1.53331\tvalid_0's multi_logloss: 1.53331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.135258358662614"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = LGBMClassifier(**lgbm_params)\n",
    "gbm.fit(\n",
    "    X_train, y_train,\n",
    "    sample_weight=weights_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_sample_weight=[weights_val],\n",
    "    eval_metric='multi_logloss',\n",
    "    early_stopping_rounds=128,\n",
    "    verbose=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb2b78376d8>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = gbm.predict(X_test)\n",
    "batch_scores = pd.Series([\n",
    "    accuracy_score(\n",
    "        y_test[batch_idx:min(y_test.shape[0], batch_idx+month_days)], \n",
    "        y_pred[batch_idx:min(y_pred.shape[0], batch_idx+month_days)]\n",
    "    ) for batch_idx in range(0,y_test.shape[0],month_days)\n",
    "])\n",
    "batch_scores.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how quickly the model performance drops when dynamics of the financial markets change beyond what the model has seen in training.\n",
    "\n",
    "This is no surprise, since all of the features are highly correlated and we have no way of capturing these long-term substantial changes right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-training with important features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(gbm.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = gbm.feature_importances_ != 0\n",
    "X_train = X_train[:,important_features]\n",
    "X_val = X_val[:,important_features]\n",
    "X_test = X_test[:,important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lgbm_params = {\n",
    "#     Learning params:\n",
    "    'num_leaves': 32,\n",
    "    'learning_rate': 0.0012137,\n",
    "    'n_estimators': 4096,\n",
    "#     Regularization:\n",
    "    'lambda_l2': 0.1,\n",
    "#     Technical:\n",
    "    'silent': True,\n",
    "    'n_jobs': 4,\n",
    "    'num_class': 18\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 128 rounds.\n",
      "[256]\tvalid_0's multi_logloss: 1.67782\tvalid_0's multi_logloss: 1.67782\n",
      "[512]\tvalid_0's multi_logloss: 1.6364\tvalid_0's multi_logloss: 1.6364\n",
      "[768]\tvalid_0's multi_logloss: 1.60431\tvalid_0's multi_logloss: 1.60431\n",
      "[1024]\tvalid_0's multi_logloss: 1.5828\tvalid_0's multi_logloss: 1.5828\n",
      "[1280]\tvalid_0's multi_logloss: 1.5552\tvalid_0's multi_logloss: 1.5552\n",
      "[1536]\tvalid_0's multi_logloss: 1.53407\tvalid_0's multi_logloss: 1.53407\n",
      "Early stopping, best iteration is:\n",
      "[1628]\tvalid_0's multi_logloss: 1.53039\tvalid_0's multi_logloss: 1.53039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', lambda_l2=0.1, learning_rate=0.0012137,\n",
       "        max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=4096, n_jobs=4, num_class=18,\n",
       "        num_leaves=32, objective=None, random_state=None, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = LGBMClassifier(**final_lgbm_params)\n",
    "gbm.fit(\n",
    "    X_train, y_train,\n",
    "    sample_weight=weights_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_sample_weight=[weights_val],\n",
    "    eval_metric='multi_logloss',\n",
    "    early_stopping_rounds=128,\n",
    "    verbose=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb2bcee95c0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = gbm.predict(X_test)\n",
    "batch_scores = pd.Series([\n",
    "    accuracy_score(\n",
    "        y_test[batch_idx:min(y_test.shape[0], batch_idx+month_days)], \n",
    "        y_pred[batch_idx:min(y_pred.shape[0], batch_idx+month_days)]\n",
    "    ) for batch_idx in range(0,y_test.shape[0],month_days)\n",
    "])\n",
    "batch_scores.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look any better, let's see how the score changes when the 2nd layer model has more data about markets themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding time series features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_aggs = ['min', 'max', 'mean', 'var', 'skew', 'kurt']\n",
    "\n",
    "def aggregated_returns(df: pd.DataFrame, aggs=returns_aggs) -> pd.DataFrame:\n",
    "    returns = (df.shift(1) - df) / df\n",
    "    return returns.agg(aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funds_df.groupby(funds_df.index.year).tail(1).agg(returns_aggs).values.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_X(\n",
    "        funds_df: pd.DataFrame,\n",
    "        min_test_idx: int=5*year_days, \n",
    "        test_len: int=year_days\n",
    "    ):\n",
    "    max_test_idx = len(funds_df)\n",
    "    n_samples = max_test_idx - min_test_idx - test_len\n",
    "    n_returns_features = funds_df.shape[1]*len(returns_aggs)\n",
    "    n_groupped_features = funds_df.shape[1]*len(returns_aggs)\n",
    "    n_features = n_returns_features + n_groupped_features\n",
    "    X = np.zeros((n_samples, n_features))\n",
    "    for idx in tqdm(range(n_samples)):\n",
    "        test_starting_idx = min_test_idx + idx\n",
    "        present_data = funds_df.iloc[:test_starting_idx]\n",
    "        X[idx,:n_returns_features] = aggregated_returns(present_data, returns_aggs).values.ravel()\n",
    "        X[idx,n_returns_features:] = present_data.agg(returns_aggs).values.ravel()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3289/3289 [01:46<00:00, 30.93it/s]\n"
     ]
    }
   ],
   "source": [
    "X_features = compute_new_X(funds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now re-train the model, once again with feature selection by applying hard l1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2631, 264), (2631,), (658, 264), (658,))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.hstack([X, X_features]), y, test_size=0.2, shuffle=False)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2104, 264), (2104,), (527, 264), (527,), (658, 264), (658,))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_train = sample_weights[:y_train.shape[0]]\n",
    "weights_val = sample_weights[y_train.shape[0]:y_train.shape[0]+y_val.shape[0]]\n",
    "weights_test = sample_weights[y_train.shape[0]+y_val.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the classes will never be predicted because they are not present in the training set.\n",
    "\n",
    "For now, we will just remove the rarely occuring values from the entire dataset and work on the ones that can be split efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  2,  3,  5,  6, 10, 11, 12, 13, 16, 17]),\n",
       " array([ 0,  3,  7, 12, 13, 16, 17]),\n",
       " array([ 0,  2,  6,  7, 12, 13, 16, 17]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train), np.unique(y_val), np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(524, 264) (524,) (524,)\n"
     ]
    }
   ],
   "source": [
    "keep_indices = np.isin(y_val, np.unique(y_train))\n",
    "X_val, y_val, weights_val = X_val[keep_indices], y_val[keep_indices], weights_val[keep_indices]\n",
    "print(X_val.shape, y_val.shape, weights_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "#     Learning params:\n",
    "    'num_leaves': 32,\n",
    "    'learning_rate': 0.0002137,\n",
    "    'n_estimators': 4096,\n",
    "#     Regularization:\n",
    "    'lambda_l1': 0.025,\n",
    "    'subsample': 0.75,\n",
    "#     Technical:\n",
    "    'silent': True,\n",
    "    'n_jobs': 12,\n",
    "    'num_class': 18\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 512 rounds.\n",
      "[256]\tvalid_0's multi_logloss: 1.7526\tvalid_0's multi_logloss: 1.7526\n",
      "[512]\tvalid_0's multi_logloss: 1.75241\tvalid_0's multi_logloss: 1.75241\n",
      "[768]\tvalid_0's multi_logloss: 1.7568\tvalid_0's multi_logloss: 1.7568\n",
      "Early stopping, best iteration is:\n",
      "[390]\tvalid_0's multi_logloss: 1.75186\tvalid_0's multi_logloss: 1.75186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', lambda_l1=0.025, learning_rate=0.0002137,\n",
       "        max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=4096, n_jobs=12, num_class=18,\n",
       "        num_leaves=32, objective=None, random_state=None, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=0.75,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = LGBMClassifier(**lgbm_params)\n",
    "gbm.fit(\n",
    "    X_train, y_train,\n",
    "    sample_weight=weights_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_sample_weight=[weights_val],\n",
    "    eval_metric='multi_logloss',\n",
    "    early_stopping_rounds=512,\n",
    "    verbose=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb2aaa82198>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = gbm.predict(X_test)\n",
    "batch_scores = pd.Series([\n",
    "    accuracy_score(\n",
    "        y_test[batch_idx:min(y_test.shape[0], batch_idx+month_days)], \n",
    "        y_pred[batch_idx:min(y_pred.shape[0], batch_idx+month_days)]\n",
    "    ) for batch_idx in range(0,y_test.shape[0],month_days)\n",
    "])\n",
    "batch_scores.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6391014317674686, 0.9374953620982656)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(np.corrcoef(X_features)), np.mean(np.corrcoef(X_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9412195422994778, 0.9827917097014088)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(np.corrcoef(X)), np.mean(np.corrcoef(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290.0722222222222, 166.77380952380952)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# old features, new features\n",
    "gbm.feature_importances_[X_features.shape[1]:].mean(), gbm.feature_importances_[:X_features.shape[1]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, the model does not take advantage of the additional features that it was given.\n",
    "\n",
    "The problem is, that most of the features are strongly correlated anyway, same goes for the base-layer PyPortfolioOpt models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing dimentionality with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 728 ms, sys: 0 ns, total: 728 ms\n",
      "Wall time: 62.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=120)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "#     Learning params:\n",
    "    'num_leaves': 32,\n",
    "    'learning_rate': 0.0002137,\n",
    "    'n_estimators': 4096,\n",
    "#     Regularization:\n",
    "    'lambda_l1': 0.025,\n",
    "    'subsample': 0.75,\n",
    "#     Technical:\n",
    "    'silent': True,\n",
    "    'n_jobs': 12,\n",
    "    'num_class': 18\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 512 rounds.\n",
      "[256]\tvalid_0's multi_logloss: 1.76753\tvalid_0's multi_logloss: 1.76753\n",
      "[512]\tvalid_0's multi_logloss: 1.77788\tvalid_0's multi_logloss: 1.77788\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 1.76212\tvalid_0's multi_logloss: 1.76212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', lambda_l1=0.025, learning_rate=0.0002137,\n",
       "        max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
       "        min_split_gain=0.0, n_estimators=4096, n_jobs=12, num_class=18,\n",
       "        num_leaves=32, objective=None, random_state=None, reg_alpha=0.0,\n",
       "        reg_lambda=0.0, silent=True, subsample=0.75,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = LGBMClassifier(**lgbm_params)\n",
    "gbm.fit(\n",
    "    X_train, y_train,\n",
    "    sample_weight=weights_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_sample_weight=[weights_val],\n",
    "    eval_metric='multi_logloss',\n",
    "    early_stopping_rounds=512,\n",
    "    verbose=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb2a54ce8d0>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = gbm.predict(X_test)\n",
    "batch_scores = pd.Series([\n",
    "    accuracy_score(\n",
    "        y_test[batch_idx:min(y_test.shape[0], batch_idx+month_days)], \n",
    "        y_pred[batch_idx:min(y_pred.shape[0], batch_idx+month_days)]\n",
    "    ) for batch_idx in range(0,y_test.shape[0],month_days)\n",
    "])\n",
    "batch_scores.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this was a total disaster.\n",
    "\n",
    "# Conclusions & next steps\n",
    "\n",
    "Next step would be to train an LSTM on these time series, which should nicely take care of correlated input.\n",
    "Additionally, we will be able to compute a custom loss that will basically optimize the return for the given maximum risk threshold.\n",
    "\n",
    "Additionally, it is important to take into account that we aimed to classify which base model performs the best, which is likely the harder task than selecting an optimal portfolio based on base-model portfolio selections.\n",
    "This, however, would be much harder to implement using gradient boosting models and we will explore this approach when building our LSTM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
