{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "\n",
    "In order to create better portfolios, we need to research best possible forecasting approach, for both the covariance matrix and the expected returns.\n",
    "\n",
    "## Covariance\n",
    "\n",
    "Baseline: histroical covariance / CovarianceShrinkage taken from PyPortfolioOpt\n",
    "\n",
    "References:\n",
    "- [Summary of classical forecasting methods](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2411493)\n",
    "- [Paper doing similar work to ours, with interesing strategies to extract longer-period values from short-period predictions](http://econ.au.dk/fileadmin/site_files/filer_oekonomi/Working_Papers/CREATES/2014/rp14_42.pdf?fbclid=IwAR1PD0w4EFzq6MfJlAlzjDVCZpXmER5sVVjM7tkIVzbogZLitIaGpgGhBY8)\n",
    "\n",
    "## Expected returns\n",
    "\n",
    "Baseline: mean historical yearly returns\n",
    "\n",
    "Notes:\n",
    "- might be unpredictable\n",
    "\n",
    "References: -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.data_loader import load_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AP</th>\n",
       "      <th>ARR</th>\n",
       "      <th>ARW</th>\n",
       "      <th>G</th>\n",
       "      <th>OP</th>\n",
       "      <th>ORR</th>\n",
       "      <th>ORW</th>\n",
       "      <th>a5.c</th>\n",
       "      <th>wig2</th>\n",
       "      <th>^aex</th>\n",
       "      <th>...</th>\n",
       "      <th>SEK</th>\n",
       "      <th>CHF</th>\n",
       "      <th>THB</th>\n",
       "      <th>TTD</th>\n",
       "      <th>TND</th>\n",
       "      <th>AED</th>\n",
       "      <th>GBP</th>\n",
       "      <th>USD</th>\n",
       "      <th>UYU</th>\n",
       "      <th>VEB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>415.90</td>\n",
       "      <td>549.11</td>\n",
       "      <td>354.45</td>\n",
       "      <td>401.26</td>\n",
       "      <td>275.08</td>\n",
       "      <td>520.13</td>\n",
       "      <td>230.72</td>\n",
       "      <td>1204.88</td>\n",
       "      <td>1852.9</td>\n",
       "      <td>675.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085771</td>\n",
       "      <td>0.456726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.115867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.726696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>404.41</td>\n",
       "      <td>533.89</td>\n",
       "      <td>357.14</td>\n",
       "      <td>401.42</td>\n",
       "      <td>275.08</td>\n",
       "      <td>520.02</td>\n",
       "      <td>229.63</td>\n",
       "      <td>1194.41</td>\n",
       "      <td>1796.6</td>\n",
       "      <td>642.25</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.465253</td>\n",
       "      <td>0.019568</td>\n",
       "      <td>0.115445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197034</td>\n",
       "      <td>1.18701</td>\n",
       "      <td>0.723608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>400.04</td>\n",
       "      <td>527.38</td>\n",
       "      <td>351.19</td>\n",
       "      <td>401.59</td>\n",
       "      <td>275.08</td>\n",
       "      <td>519.22</td>\n",
       "      <td>229.22</td>\n",
       "      <td>1192.89</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>632.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086740</td>\n",
       "      <td>0.466615</td>\n",
       "      <td>0.019422</td>\n",
       "      <td>0.115510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197039</td>\n",
       "      <td>1.18624</td>\n",
       "      <td>0.723627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AP     ARR     ARW       G      OP     ORR     ORW     a5.c  \\\n",
       "2000-01-01     NaN     NaN     NaN     NaN     NaN     NaN     NaN      NaN   \n",
       "2000-01-02     NaN     NaN     NaN     NaN     NaN     NaN     NaN      NaN   \n",
       "2000-01-03  415.90  549.11  354.45  401.26  275.08  520.13  230.72  1204.88   \n",
       "2000-01-04  404.41  533.89  357.14  401.42  275.08  520.02  229.63  1194.41   \n",
       "2000-01-05  400.04  527.38  351.19  401.59  275.08  519.22  229.22  1192.89   \n",
       "\n",
       "              wig2    ^aex  ...       SEK       CHF       THB       TTD  TND  \\\n",
       "2000-01-01     NaN     NaN  ...       NaN       NaN       NaN       NaN  NaN   \n",
       "2000-01-02     NaN     NaN  ...       NaN       NaN       NaN       NaN  NaN   \n",
       "2000-01-03  1852.9  675.44  ...  0.085771  0.456726       NaN  0.115867  NaN   \n",
       "2000-01-04  1796.6  642.25  ...       NaN  0.465253  0.019568  0.115445  NaN   \n",
       "2000-01-05  1777.0  632.31  ...  0.086740  0.466615  0.019422  0.115510  NaN   \n",
       "\n",
       "                 AED      GBP       USD  UYU       VEB  \n",
       "2000-01-01       NaN      NaN       NaN  NaN       NaN  \n",
       "2000-01-02       NaN      NaN       NaN  NaN       NaN  \n",
       "2000-01-03  0.197875      NaN  0.726696  NaN       NaN  \n",
       "2000-01-04  0.197034  1.18701  0.723608  NaN  0.001114  \n",
       "2000-01-05  0.197039  1.18624  0.723627  NaN  0.001114  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AP</th>\n",
       "      <th>ARR</th>\n",
       "      <th>ARW</th>\n",
       "      <th>G</th>\n",
       "      <th>OP</th>\n",
       "      <th>ORR</th>\n",
       "      <th>ORW</th>\n",
       "      <th>a5.c</th>\n",
       "      <th>wig2</th>\n",
       "      <th>^aex</th>\n",
       "      <th>...</th>\n",
       "      <th>SEK</th>\n",
       "      <th>CHF</th>\n",
       "      <th>THB</th>\n",
       "      <th>TTD</th>\n",
       "      <th>TND</th>\n",
       "      <th>AED</th>\n",
       "      <th>GBP</th>\n",
       "      <th>USD</th>\n",
       "      <th>UYU</th>\n",
       "      <th>VEB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>415.90</td>\n",
       "      <td>549.11</td>\n",
       "      <td>354.45</td>\n",
       "      <td>401.26</td>\n",
       "      <td>275.08</td>\n",
       "      <td>520.13</td>\n",
       "      <td>230.72</td>\n",
       "      <td>1204.88</td>\n",
       "      <td>1852.9</td>\n",
       "      <td>675.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085771</td>\n",
       "      <td>0.456726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.115867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.726696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>404.41</td>\n",
       "      <td>533.89</td>\n",
       "      <td>357.14</td>\n",
       "      <td>401.42</td>\n",
       "      <td>275.08</td>\n",
       "      <td>520.02</td>\n",
       "      <td>229.63</td>\n",
       "      <td>1194.41</td>\n",
       "      <td>1796.6</td>\n",
       "      <td>642.25</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.465253</td>\n",
       "      <td>0.019568</td>\n",
       "      <td>0.115445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197034</td>\n",
       "      <td>1.18701</td>\n",
       "      <td>0.723608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>400.04</td>\n",
       "      <td>527.38</td>\n",
       "      <td>351.19</td>\n",
       "      <td>401.59</td>\n",
       "      <td>275.08</td>\n",
       "      <td>519.22</td>\n",
       "      <td>229.22</td>\n",
       "      <td>1192.89</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>632.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086740</td>\n",
       "      <td>0.466615</td>\n",
       "      <td>0.019422</td>\n",
       "      <td>0.115510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197039</td>\n",
       "      <td>1.18624</td>\n",
       "      <td>0.723627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>410.15</td>\n",
       "      <td>522.02</td>\n",
       "      <td>347.96</td>\n",
       "      <td>401.75</td>\n",
       "      <td>275.07</td>\n",
       "      <td>519.62</td>\n",
       "      <td>228.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1832.1</td>\n",
       "      <td>624.21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.468650</td>\n",
       "      <td>0.019427</td>\n",
       "      <td>0.115662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197260</td>\n",
       "      <td>1.19474</td>\n",
       "      <td>0.724439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>429.16</td>\n",
       "      <td>533.16</td>\n",
       "      <td>351.87</td>\n",
       "      <td>401.93</td>\n",
       "      <td>275.07</td>\n",
       "      <td>520.80</td>\n",
       "      <td>230.09</td>\n",
       "      <td>1223.61</td>\n",
       "      <td>1933.2</td>\n",
       "      <td>644.86</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.465233</td>\n",
       "      <td>0.019410</td>\n",
       "      <td>0.115876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197989</td>\n",
       "      <td>1.19596</td>\n",
       "      <td>0.727113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AP     ARR     ARW       G      OP     ORR     ORW     a5.c  \\\n",
       "2000-01-03  415.90  549.11  354.45  401.26  275.08  520.13  230.72  1204.88   \n",
       "2000-01-04  404.41  533.89  357.14  401.42  275.08  520.02  229.63  1194.41   \n",
       "2000-01-05  400.04  527.38  351.19  401.59  275.08  519.22  229.22  1192.89   \n",
       "2000-01-06  410.15  522.02  347.96  401.75  275.07  519.62  228.82      NaN   \n",
       "2000-01-07  429.16  533.16  351.87  401.93  275.07  520.80  230.09  1223.61   \n",
       "\n",
       "              wig2    ^aex  ...       SEK       CHF       THB       TTD  TND  \\\n",
       "2000-01-03  1852.9  675.44  ...  0.085771  0.456726       NaN  0.115867  NaN   \n",
       "2000-01-04  1796.6  642.25  ...       NaN  0.465253  0.019568  0.115445  NaN   \n",
       "2000-01-05  1777.0  632.31  ...  0.086740  0.466615  0.019422  0.115510  NaN   \n",
       "2000-01-06  1832.1  624.21  ...       NaN  0.468650  0.019427  0.115662  NaN   \n",
       "2000-01-07  1933.2  644.86  ...       NaN  0.465233  0.019410  0.115876  NaN   \n",
       "\n",
       "                 AED      GBP       USD  UYU       VEB  \n",
       "2000-01-03  0.197875      NaN  0.726696  NaN       NaN  \n",
       "2000-01-04  0.197034  1.18701  0.723608  NaN  0.001114  \n",
       "2000-01-05  0.197039  1.18624  0.723627  NaN  0.001114  \n",
       "2000-01-06  0.197260  1.19474  0.724439  NaN  0.001115  \n",
       "2000-01-07  0.197989  1.19596  0.727113  NaN  0.001118  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[~df['AP'].isna()]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4801, 200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling up missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filled = df.fillna(method='ffill')\n",
    "bad_cols = [col for col in filled.columns if filled[col].loc['2001-01-01':].isna().sum() > 0]\n",
    "len(bad_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features have plenty of missing data. We will fill them back, but it is important to remember that we cannot leak the back-filled data into the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_leak_after = '2005-01-01'\n",
    "cols_to_drop = [col for col in filled.columns if filled[col].loc[dont_leak_after:].isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = filled.drop(columns=cols_to_drop).fillna(method='bfill')\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that were not filled \"forward\" will be filled backward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating future covariance for forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using shrinkage to estimate future covariance matrix, we will use supervised learning to train a model that predicts covariance 1 year ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_aggs = ['min', 'max', 'mean', 'skew', 'kurt']\n",
    "\n",
    "def aggregated_returns(df: pd.DataFrame, aggs=returns_aggs) -> pd.DataFrame:\n",
    "    returns = (df.shift(1) - df) / df\n",
    "    return returns.agg(aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.12 s, sys: 4 ms, total: 4.12 s\n",
      "Wall time: 359 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dfs_to_agg_returns = [\n",
    "#     df.groupby([df.index.year, df.index.month]).tail(1),\n",
    "    df.groupby([df.index.year, df.index.weekofyear]).tail(1),\n",
    "]\n",
    "\n",
    "agg_returns = [aggregated_returns(aggregated_df) for aggregated_df in tqdm(dfs_to_agg_returns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 0 ns, total: 1.15 s\n",
      "Wall time: 78.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dfs_to_cov = [\n",
    "    df,\n",
    "    df.groupby([df.index.year, df.index.month]).tail(1),\n",
    "    df.groupby([df.index.year, df.index.weekofyear]).tail(1),\n",
    "]\n",
    "\n",
    "covs = [cov_df[fund_colnames].cov() for cov_df in dfs_to_cov]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.hstack([\n",
    "#     pd.concat(agg_returns, axis='columns', ignore_index=True).values.ravel(), \n",
    "    pd.concat(covs, axis='columns', ignore_index=True).values.ravel()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_colnames = ['AP', 'ARR', 'ARW', 'G', 'OP', 'ORR', 'ORW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _single_X_y(cleaned_df, interval_start, window_size, target_size) -> Tuple[np.array, np.array]:\n",
    "    timestep_present = cleaned_df.iloc[interval_start:interval_start+window_size]\n",
    "    timestep_future = cleaned_df.iloc[interval_start+window_size:interval_start+window_size+target_size]\n",
    "    dfs_to_cov = [\n",
    "        timestep_present,\n",
    "        timestep_present.groupby([timestep_present.index.year, timestep_present.index.month]).tail(1),\n",
    "        timestep_present.groupby([timestep_present.index.year, timestep_present.index.weekofyear]).tail(1),\n",
    "    ]\n",
    "    covs = [cov_df[fund_colnames].cov() for cov_df in dfs_to_cov]\n",
    "#     dfs_to_agg_returns = [\n",
    "#         timestep_present.groupby([timestep_present.index.year, timestep_present.index.month]).tail(1),\n",
    "#         timestep_present.groupby([timestep_present.index.year, timestep_present.index.weekofyear]).tail(1),\n",
    "#     ]\n",
    "#     agg_returns = [aggregated_returns(aggregated_df) for aggregated_df in tqdm(dfs_to_agg_returns)]\n",
    "    features = np.hstack([\n",
    "#         pd.concat(agg_returns, axis='columns', ignore_index=True).values.ravel(), \n",
    "        pd.concat(covs, axis='columns', ignore_index=True).values.ravel()\n",
    "    ])\n",
    "    target = timestep_future[fund_colnames].cov().values.ravel()\n",
    "    return features, target\n",
    "\n",
    "def calculate_X_y(cleaned_df: pd.DataFrame, window_size: int=750, target_size: int=250, n_features=147) -> Tuple[np.array, np.array]:\n",
    "    n_samples = len(cleaned_df)-window_size-target_size\n",
    "    X = np.zeros((n_samples, n_features))\n",
    "    y = np.zeros((n_samples, 7**2))\n",
    "    for interval_start in tqdm(range(n_samples)):\n",
    "        features, target = _single_X_y(cleaned_df, interval_start, window_size, target_size)\n",
    "        X[interval_start] = features\n",
    "        y[interval_start] = target\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3801/3801 [00:52<00:00, 72.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 29s, sys: 316 ms, total: 17min 29s\n",
      "Wall time: 52.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = calculate_X_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3801, 147)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3801, 49)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not shuffling during the test set separation in order not to let the model know about macro trends that might appear in the future and are not captured within our set of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3040, 147), (3040, 49), (761, 147), (761, 49))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2432, 147), (2432, 49), (608, 147), (608, 49), (761, 147), (761, 49))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 4096,\n",
    "    'silent': True,\n",
    "    'n_jobs': 8,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable 1 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 2.2516\n",
      "Target variable 2 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 21.3792\n",
      "Target variable 3 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 4.2642\n",
      "Target variable 4 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 190.2276\n",
      "Target variable 5 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 69.0620\n",
      "Target variable 6 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 109.4461\n",
      "Target variable 7 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 16.4769\n",
      "Target variable 8 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 21.3792\n",
      "Target variable 9 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 3.7277\n",
      "Target variable 10 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 15.5864\n",
      "Target variable 11 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 211.9990\n",
      "Target variable 12 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 103.0813\n",
      "Target variable 13 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 9037.9951\n",
      "Target variable 14 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 48.1705\n",
      "Target variable 15 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 4.2642\n",
      "Target variable 16 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 15.5864\n",
      "Target variable 17 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 1.0807\n",
      "Target variable 18 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 112.3408\n",
      "Target variable 19 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 85.7386\n",
      "Target variable 20 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 98.6293\n",
      "Target variable 21 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 9.6865\n",
      "Target variable 22 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 190.2276\n",
      "Target variable 23 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 211.9990\n",
      "Target variable 24 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 112.3408\n",
      "Target variable 25 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 695.7958\n",
      "Target variable 26 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 473.2059\n",
      "Target variable 27 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 1052.5332\n",
      "Target variable 28 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 480.0622\n",
      "Target variable 29 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 69.0620\n",
      "Target variable 30 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 103.0813\n",
      "Target variable 31 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 85.7386\n",
      "Target variable 32 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 473.2059\n",
      "Target variable 33 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 52.1387\n",
      "Target variable 34 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 409.5012\n",
      "Target variable 35 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 332.6465\n",
      "Target variable 36 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 109.4461\n",
      "Target variable 37 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 9037.9951\n",
      "Target variable 38 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 98.6293\n",
      "Target variable 39 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 1052.5332\n",
      "Target variable 40 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 409.5012\n",
      "Target variable 41 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 129.4966\n",
      "Target variable 42 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 188.9734\n",
      "Target variable 43 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 16.4769\n",
      "Target variable 44 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 48.1705\n",
      "Target variable 45 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 9.6865\n",
      "Target variable 46 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 480.0622\n",
      "Target variable 47 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 332.6465\n",
      "Target variable 48 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 188.9734\n",
      "Target variable 49 out of 49\n",
      "Training until validation scores don't improve for 64 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's mape: 2.6914\tvalid_0's l2: 4.92006e+06\n",
      "Test set error: 16.3128\n",
      "CPU times: user 6min 32s, sys: 188 ms, total: 6min 32s\n",
      "Wall time: 49.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "models = {}\n",
    "test_errors = {}\n",
    "n_targets = y_train.shape[1]\n",
    "for target_idx in range(n_targets):\n",
    "    print(f\"Target variable {target_idx+1} out of {n_targets}\")\n",
    "    train_target = y_train[:,target_idx]\n",
    "    val_target = y_val[:,target_idx]\n",
    "    test_target = y_test[:,target_idx]\n",
    "    gbm = LGBMRegressor(**lgbm_params)\n",
    "    gbm.fit(\n",
    "        X_train, y_train[:,0],\n",
    "        eval_set=[(X_val, y_val[:,0])],\n",
    "        eval_metric='mape',\n",
    "        early_stopping_rounds=64,\n",
    "        verbose=512\n",
    "    )\n",
    "    y_pred = gbm.predict(X_test)\n",
    "    test_err = mean_absolute_percentage_error(y_test[:,target_idx], y_pred)\n",
    "    print(f\"Test set error: {test_err:.4f}\")\n",
    "    models[target_idx] = gbm\n",
    "    test_errors[target_idx] = test_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the error rates are pretty bad and forecasting covariance using this set of data is not the greatest idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Shrunk covariance matrix as feature to the model\n",
    "Instead of forecasting the correct future covariance for the portfolio, we will train the model as a 2nd-stage estimator, stacked on predictions from existing widely used covariance shrinkage techniques.\n",
    "\n",
    "To do this, we will train a fully-connected neural net with a loss that corresponds to future portfolio performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
